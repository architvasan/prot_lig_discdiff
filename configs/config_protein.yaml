# Protein Discrete Diffusion Training Configuration
# Optimized for protein sequences with SUBS loss

# Vocabulary: 25 base tokens (20 amino acids + 5 special) + 1 absorbing = 26 total
tokens: 26
devicetype: xpu  # Changed to xpu for Aurora, use cuda for other systems

# Model configuration - scaled up for better protein sequence modeling
model:
  dim: 2048                    # Model dimension (scaled 2x: 1024 â†’ 2048 for better capacity)
  n_heads: 32                  # Number of attention heads (scaled to maintain head_dim=64)
  n_layers: 20                 # Number of transformer layers (increased from 16)
  max_seq_len: 512           # Maximum protein sequence length
  cond_dim: 512              # Time embedding dimension (scaled with model dim)
  scale_by_sigma: false      # Disable for SUBS loss (recommended)
  dropout: 0.1               # Dropout for regularization (helps with larger models)
  mlp_ratio: 4               # MLP expansion ratio (standard for transformers)

# Training configuration - optimized for 2048d model
training:
  batch_size: 4             # Reduced batch size for 2048d model (4 * 240 = 960 per step)
  learning_rate: 6.0e-6        # Reduced LR for larger model stability
  weight_decay: 0.03         # Increased weight decay for 400M parameter model
  warmup_steps: 5000         # Longer warmup for larger model stability
  max_steps: 500000000           # Total training steps
  gradient_clip_norm: 1.0    # Gradient clipping for larger model
  accumulate_grad_batches: 8 # Effective batch = 4 * 8 * 240 = 7,680 per step
  use_ema: true              # Use exponential moving average
  ema_decay: 0.9999          # EMA decay rate
  use_subs_loss: true        # Use SUBS loss (recommended for stability)
  use_mixed_precision: false # DISABLED: Mixed precision can cause NaN values in discrete diffusion
  # Sampling failure handling
  max_sampling_failures: 1   # Maximum consecutive sampling failures before stopping
  stop_on_sampling_failure: true  # Whether to stop training on sampling failures
  num_epochs: 10           # Number of epochs (will stop after max_steps)

# Data configuration
data:
  max_length: 512            # Maximum sequence length
  tokenize_on_fly: true     # Set to true if data is not pre-tokenized
  use_streaming: true       # Set to true for very large datasets
  num_workers: 0             # Data loading workers
  pin_memory: true           # Pin memory for faster GPU transfer

  # Data split ratios (must sum to 1.0)
  train_ratio: 0.8          # Training set ratio
  val_ratio: 0.1            # Validation set ratio
  test_ratio: 0.1           # Test set ratio
  split_seed: 42            # Seed for reproducible data splits

# Time sampling stochasticity control
time_sampling:
  mode: "enhanced_stochastic"  # Options: "deterministic", "rank_based", "enhanced_stochastic", "full_random"
  extra_entropy_sources: true  # Use additional entropy sources (batch_idx, sequence_idx, etc.)
  temporal_mixing: true        # Mix current and previous step information
  sequence_level_noise: true   # Add per-sequence randomness within batches
  stochasticity_scale: 1.0     # Scale factor for stochasticity (0.0 = deterministic, 2.0 = very random)

# Noise schedule - Cosine works well for proteins
noise:
  type: "cosine"          # Noise schedule type
  eps: 1e-3                  # Small epsilon for numerical stability
  sigma_min: 1e-3            # Minimum noise level
  sigma_max: 1.5             # Maximum noise level (reduced from 2.0 for better stability)

# Curriculum learning - DISABLED for debugging
curriculum:
  enabled: false             # DISABLED: Turn off curriculum learning
  start_bias: 0.8            # Start with 80% bias towards low noise
  end_bias: 0.0              # End with uniform sampling
  decay_steps: 15000         # Decay over 15k steps (30% of training)

# Logging and checkpointing
logging:
  log_interval: 100           # Log metrics to wandb every N steps (frequent logging, no performance impact)
  debug_interval: 100        # Print debug information every N steps (sigma stats, etc.)
  checkpoint_interval: 1000  # Checkpoint every 1000 steps (DEPRECATED - use validation.checkpoint_freq)
  eval_interval: 500         # Validation evaluation every 500 steps (DEPRECATED - use validation.eval_freq)

# Validation and checkpointing
validation:
  eval_freq: 500             # Run validation every 500 steps
  checkpoint_freq: 1000      # Save checkpoint every 1000 steps
  checkpoint_on_improvement: true  # Only save checkpoint if validation improves
  patience: 50               # Number of evaluations without improvement before stopping
  min_delta: 0.001          # Minimum improvement to consider as better
  save_best_only: false     # Whether to only save the best model
  val_batch_limit: 20       # Limit validation to N batches for speed

# Optimizer settings (AdamW with protein-optimized settings)
optimizer:
  type: "adamw"              # AdamW optimizer
  betas: [0.9, 0.95]         # Beta parameters (slightly different beta2)
  eps: 1e-8                  # Epsilon for numerical stability

# Learning rate scheduler (cosine with warmup)
scheduler:
  type: "cosine"             # Cosine annealing
  min_lr_ratio: 0.1          # Minimum LR = 10% of max LR

# Sampling configuration for monitoring during training
sampling:
  sample_interval: 1000      # Sample every 1000 steps (reduced from 100 to avoid performance degradation)
  min_steps_before_sampling: 1000  # Wait this many steps before starting sampling (model stability)
  eval_batch_size: 4         # Number of sequences to sample per rank (reduced from 10 for speed)
  eval_max_length: 256       # Max length for evaluation samples (reduced from 512 for speed)
  eval_steps: 50             # Number of sampling steps (reduced from 100 for speed)
  predictor: "euler"         # Sampling predictor (analytic is fastest)
  save_to_file: true         # Save sampled sequences to file
  calculate_esm_perplexity: true  # Calculate ESM perplexity for generated sequences
  esm_model: "esm2_t6_8M_UR50D"   # ESM model for perplexity calculation
  esm_batch_size: 2          # Batch size for ESM evaluation (reduced for speed)
  use_subs_sampling: true    # Use SUBS parameterization during sampling (recommended for consistency with training)
