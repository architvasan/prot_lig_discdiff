# Protein Discrete Diffusion Training Configuration
# Optimized for protein sequences with SUBS loss

# Vocabulary: 25 base tokens (20 amino acids + 5 special) + 1 absorbing = 26 total
tokens: 26
devicetype: xpu  # Changed to xpu for Aurora, use cuda for other systems

# Model configuration - balanced for protein sequences
model:
  dim: 1024                    # Model dimension (smaller for faster training)
  n_heads: 16                  # Number of attention heads
  n_layers: 16                 # Number of transformer layers
  max_seq_len: 512           # Maximum protein sequence length
  cond_dim: 256              # Time embedding dimension
  scale_by_sigma: false      # Disable for SUBS loss (recommended)

# Training configuration - optimized for stability
training:
  batch_size: 8             # Mini-batch size (adjust based on memory)
  learning_rate: 1.0e-5        # Learning rate (reduced for numerical stability)
  weight_decay: 0.01         # Weight decay for regularization
  warmup_steps: 2000         # Warmup steps (longer for stability)
  max_steps: 500000000           # Total training steps
  gradient_clip_norm: 0.5    # Gradient clipping (reduced for stability)
  accumulate_grad_batches: 4 # Effective batch = 16 * 8 = 128
  use_ema: true              # Use exponential moving average
  ema_decay: 0.9999          # EMA decay rate
  use_subs_loss: true        # Use SUBS loss (recommended for stability)
  use_mixed_precision: false # DISABLED: Mixed precision can cause NaN values in discrete diffusion
  # Sampling failure handling
  max_sampling_failures: 1   # Maximum consecutive sampling failures before stopping
  stop_on_sampling_failure: true  # Whether to stop training on sampling failures
  num_epochs: 10           # Number of epochs (will stop after max_steps)

# Data configuration
data:
  max_length: 512            # Maximum sequence length
  tokenize_on_fly: true     # Set to true if data is not pre-tokenized
  use_streaming: true       # Set to true for very large datasets
  num_workers: 0             # Data loading workers
  pin_memory: true           # Pin memory for faster GPU transfer

  # Data split ratios (must sum to 1.0)
  train_ratio: 0.8          # Training set ratio
  val_ratio: 0.1            # Validation set ratio
  test_ratio: 0.1           # Test set ratio
  split_seed: 42            # Seed for reproducible data splits

# Time sampling stochasticity control
time_sampling:
  mode: "enhanced_stochastic"  # Options: "deterministic", "rank_based", "enhanced_stochastic", "full_random"
  extra_entropy_sources: true  # Use additional entropy sources (batch_idx, sequence_idx, etc.)
  temporal_mixing: true        # Mix current and previous step information
  sequence_level_noise: true   # Add per-sequence randomness within batches
  stochasticity_scale: 1.0     # Scale factor for stochasticity (0.0 = deterministic, 2.0 = very random)

# Noise schedule - Cosine works well for proteins
noise:
  type: "cosine"          # Noise schedule type
  eps: 1e-3                  # Small epsilon for numerical stability
  sigma_min: 1e-4            # Minimum noise level
  sigma_max: 1.5             # Maximum noise level (reduced from 2.0 for better stability)

# Curriculum learning - DISABLED for debugging
curriculum:
  enabled: false             # DISABLED: Turn off curriculum learning
  start_bias: 0.8            # Start with 80% bias towards low noise
  end_bias: 0.0              # End with uniform sampling
  decay_steps: 15000         # Decay over 15k steps (30% of training)

# Logging and checkpointing
logging:
  log_interval: 100          # Log every 100 steps
  checkpoint_interval: 1000  # Checkpoint every 1000 steps
  eval_interval: 500         # Validation evaluation every 500 steps

# Validation and checkpointing
validation:
  eval_freq: 500             # Run validation every 500 steps
  checkpoint_freq: 1000      # Save checkpoint every 1000 steps
  checkpoint_on_improvement: true  # Only save checkpoint if validation improves
  patience: 10               # Number of evaluations without improvement before stopping
  min_delta: 0.001          # Minimum improvement to consider as better
  save_best_only: false     # Whether to only save the best model
  val_batch_limit: 20       # Limit validation to N batches for speed

# Optimizer settings (AdamW with protein-optimized settings)
optimizer:
  type: "adamw"              # AdamW optimizer
  betas: [0.9, 0.95]         # Beta parameters (slightly different beta2)
  eps: 1e-8                  # Epsilon for numerical stability

# Learning rate scheduler (cosine with warmup)
scheduler:
  type: "cosine"             # Cosine annealing
  min_lr_ratio: 0.1          # Minimum LR = 10% of max LR

# Sampling configuration for monitoring during training
sampling:
  sample_interval: 100       # Sample every 100 steps
  min_steps_before_sampling: 100  # Wait this many steps before starting sampling (model stability)
  eval_batch_size: 10        # Number of sequences to sample per rank (increased from 4 to 10)
  eval_max_length: 512       # Max length for evaluation samples (shorter for speed)
  eval_steps: 100            # Number of sampling steps (fewer for speed)
  predictor: "euler"         # Sampling predictor (analytic is fastest)
  save_to_file: true         # Save sampled sequences to file
  calculate_esm_perplexity: true  # Calculate ESM perplexity for generated sequences
  esm_model: "esm2_t6_8M_UR50D"   # ESM model for perplexity calculation
  esm_batch_size: 4          # Batch size for ESM evaluation
  use_subs_sampling: true    # Use SUBS parameterization during sampling (recommended for consistency with training)
